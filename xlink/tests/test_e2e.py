#!/usr/bin/env python3
"""
End-to-end test for the complete pipeline workflow
This script tests the main.py functionality and validates TASK.md requirements
"""

import sys
from pathlib import Path
import pandas as pd
import json
import subprocess
import tempfile
import shutil

def test_e2e_pipeline():
    """End-to-end test of the complete pipeline"""
    print("üöÄ Starting End-to-End Pipeline Test")
    print("=" * 60)
    
    # Change to project directory
    project_dir = Path(__file__).parent.parent
    print(f"Project directory: {project_dir}")
    
    # Check if customer_churn.csv exists
    data_file = project_dir / "customer_churn.csv"
    if not data_file.exists():
        print("‚ùå customer_churn.csv not found!")
        return False
    
    print(f"‚úÖ Data file found: {data_file}")
    
    # Test 1: Run data processing
    print("\nüìä Testing Data Processing...")
    result = subprocess.run([
        sys.executable, "pipeline/main.py", 
        "--data", str(data_file), 
        "--process"
    ], cwd=project_dir, capture_output=True, text=True)
    
    if result.returncode != 0:
        print(f"‚ùå Data processing failed: {result.stderr}")
        return False
    
    print("‚úÖ Data processing completed")
    print(result.stdout[-200:])  # Last 200 chars of output
    
    # Test 2: Run model training
    print("\nü§ñ Testing Model Training...")
    result = subprocess.run([
        sys.executable, "pipeline/main.py", 
        "--train"
    ], cwd=project_dir, capture_output=True, text=True)
    
    if result.returncode != 0:
        print(f"‚ùå Model training failed: {result.stderr}")
        return False
    
    print("‚úÖ Model training completed")
    print(result.stdout[-200:])
    
    # Test 3: Run evaluation
    print("\nüìà Testing Model Evaluation...")
    result = subprocess.run([
        sys.executable, "pipeline/main.py", 
        "--evaluate"
    ], cwd=project_dir, capture_output=True, text=True)
    
    if result.returncode != 0:
        print(f"‚ùå Model evaluation failed: {result.stderr}")
        return False
    
    print("‚úÖ Model evaluation completed")
    print(result.stdout[-200:])
    
    # Test 4: Run k-fold validation
    print("\nüîÑ Testing K-Fold Validation...")
    result = subprocess.run([
        sys.executable, "pipeline/main.py", 
        "--kfold"
    ], cwd=project_dir, capture_output=True, text=True)
    
    if result.returncode != 0:
        print(f"‚ùå K-fold validation failed: {result.stderr}")
        return False
    
    print("‚úÖ K-fold validation completed")
    print(result.stdout[-200:])
    
    # Test 5: Verify outputs exist
    print("\nüìÅ Verifying Pipeline Outputs...")
    
    # Check processed data
    processed_path = project_dir / "data" / "processed" / "processed_latest.pkl"
    if processed_path.exists():
        print("‚úÖ Processed data file exists")
    else:
        print("‚ùå Processed data file missing")
        return False
    
    # Check model
    model_path = project_dir / "models" / "model_latest.pkl"
    if model_path.exists():
        print("‚úÖ Model file exists")
    else:
        print("‚ùå Model file missing")
        return False
    
    # Check evaluation results
    eval_path = project_dir / "results" / "evaluation_latest.json"
    if eval_path.exists():
        print("‚úÖ Evaluation results exist")
        
        # Load and display key metrics
        with open(eval_path, 'r') as f:
            eval_data = json.load(f)
        
        metrics = eval_data['metrics']
        print(f"\nüìä Final Model Performance:")
        print(f"   ROC-AUC: {metrics['roc_auc']:.4f}")
        print(f"   PR-AUC: {metrics['pr_auc']:.4f}")
        print(f"   Accuracy: {metrics['accuracy']:.4f}")
        print(f"   Precision: {metrics['precision']:.4f}")
        print(f"   Recall: {metrics['recall']:.4f}")
        print(f"   F1-Score: {metrics['f1_score']:.4f}")
        print(f"   Brier Score: {metrics['brier_score']:.4f}")
        
        # Validate TASK.md requirements
        print(f"\n‚úÖ TASK.md Evaluation Criteria:")
        print(f"   ROC-AUC > 0.85: {'‚úÖ' if metrics['roc_auc'] > 0.85 else '‚ö†Ô∏è'} ({metrics['roc_auc']:.4f})")
        print(f"   PR-AUC > 0.65: {'‚úÖ' if metrics['pr_auc'] > 0.65 else '‚ö†Ô∏è'} ({metrics['pr_auc']:.4f})")
        print(f"   Brier Score < 0.15: {'‚úÖ' if metrics['brier_score'] < 0.15 else '‚ö†Ô∏è'} ({metrics['brier_score']:.4f})")
        
    else:
        print("‚ùå Evaluation results missing")
        return False
    
    print("\nüéâ End-to-End Pipeline Test PASSED!")
    return True

def test_full_workflow():
    """Test the complete workflow using the --all flag"""
    print("\nüî• Testing Complete Workflow (--all flag)")
    print("=" * 60)
    
    project_dir = Path(__file__).parent.parent
    data_file = project_dir / "customer_churn.csv"
    
    if not data_file.exists():
        print("‚ùå customer_churn.csv not found for full workflow test!")
        return False
    
    # Run complete pipeline
    result = subprocess.run([
        sys.executable, "pipeline/main.py", 
        "--data", str(data_file),
        "--all"
    ], cwd=project_dir, capture_output=True, text=True)
    
    if result.returncode != 0:
        print(f"‚ùå Full workflow failed: {result.stderr}")
        print(f"Stdout: {result.stdout}")
        return False
    
    print("‚úÖ Complete workflow finished successfully")
    print("üìã Output summary:")
    print(result.stdout[-500:])  # Last 500 chars
    
    return True

def analyze_task_requirements():
    """Analyze the results against TASK.md requirements"""
    print("\nüìã TASK.MD REQUIREMENTS ANALYSIS")
    print("=" * 60)
    
    project_dir = Path(__file__).parent.parent
    
    # Check data exploration outputs
    data_exploration_dir = project_dir / "data-exploration" / "output"
    if data_exploration_dir.exists():
        print("‚úÖ Part A - Data Exploration: COMPLETED")
        print("   - EDA with data quality analysis")
        print("   - Feature preparation decisions documented")
        print("   - Statistical summaries in multiple formats")
    
    # Check modeling outputs
    results_dir = project_dir / "results"
    if results_dir.exists() and (results_dir / "evaluation_latest.json").exists():
        print("‚úÖ Part B - Modeling: COMPLETED")
        
        with open(results_dir / "evaluation_latest.json", 'r') as f:
            eval_data = json.load(f)
        
        metrics = eval_data['metrics']
        print(f"   - Model metrics calculated:")
        print(f"     * ROC-AUC: {metrics['roc_auc']:.4f}")
        print(f"     * PR-AUC: {metrics['pr_auc']:.4f}")
        print(f"     * Calibration (Brier): {metrics['brier_score']:.4f}")
        print(f"   - Segment analysis available")
    
    # Check API implementation
    api_file = project_dir / "api" / "app.py"
    if api_file.exists():
        print("‚úÖ Part D - Serving: COMPLETED")
        print("   - FastAPI service implemented")
        print("   - Multiple endpoints available")
        print("   - Docker configuration provided")
    
    # Part C is covered in PLAN.md
    plan_file = project_dir / "PLAN.md"
    if plan_file.exists():
        print("‚úÖ Part C - Deployment & Monitoring: DOCUMENTED")
        print("   - Deployment strategy outlined")
        print("   - Monitoring approach defined")
        print("   - Retraining triggers specified")

def main():
    """Main test execution"""
    print("üß™ COMPREHENSIVE PIPELINE TESTING")
    print("=" * 60)
    
    # Run individual workflow test
    success_e2e = test_e2e_pipeline()
    
    # Run full workflow test
    success_full = test_full_workflow()
    
    # Analyze requirements
    analyze_task_requirements()
    
    # Final summary
    print("\nüìä TEST SUMMARY")
    print("=" * 60)
    print(f"End-to-End Test: {'‚úÖ PASSED' if success_e2e else '‚ùå FAILED'}")
    print(f"Full Workflow Test: {'‚úÖ PASSED' if success_full else '‚ùå FAILED'}")
    
    overall_success = success_e2e and success_full
    print(f"\nOverall: {'üéâ ALL TESTS PASSED' if overall_success else '‚ö†Ô∏è SOME TESTS FAILED'}")
    
    return overall_success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)